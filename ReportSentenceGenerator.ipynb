{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8275c823",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Utilizing Markov Chains with the N-Gram Method to Suggest New Sentences from a Base Text</h1>\n",
    "\n",
    "<br>\n",
    "\n",
    "<b>Project:</b> Markov Chain and N-Grams  \n",
    "<b>Class:</b> Cpts 315 Washington State University  \n",
    "<b>Description:</b> Final Project  \n",
    "<b>By:</b> Kyle Hurd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9f2ee7",
   "metadata": {},
   "source": [
    "# Introduction - Markov Chain\n",
    "\n",
    "My motivation for designing a Sentence Generator was first inspired by a concept I read when reading about how the Google Definition `Word Usage Over Time` was implemented.\n",
    "Google provides a feature where when you look up the definition of a word, it provides a chart showing said word's usage over a period of time. I then read about how Google\n",
    "used a method called N-Grams to help determine this. This then led me to the concept where one would be able to predict the usage of a word based on the previous words leading up\n",
    "to it. I was initially going to try and apply this concept onto an infitinite supply of Tweets from Twitter, but I was not given permission by Twitter to use their API. So I thought\n",
    "of a different way to apply this N-Gram concept by reading a source text such as a book or article. From analyzing said article, I could generate new sentences that are similar\n",
    "to the source text.  \n",
    "\n",
    "The task I have laid out is to read text from a source text and be able to generate unique combinations of words based upon the original text. The sentences must be able to diverge\n",
    "from the source text (not be identical) which means we will most likely need a lot of source text, preferably matching a similar genre and writing style. Some of the challenges is\n",
    "determining a concrete and understandable method to deliver words that are connected to a certain n-gram, how to measure probabilty that a connected word should be selected over\n",
    "another, and how to make sure the sentences are grammatically correct.\n",
    "\n",
    "<p align=\"center\">\n",
    "\n",
    "<img src=\"imgs/word_usage_chart.png\"/>\n",
    "\n",
    "</p>\n",
    "\n",
    "A <b>Markov Chain</b> is a model that makes predictions based on a sequence of potential states. It will weigh the probability in which a set of states will be in the sequence and use this information to generate a new sequence. The defining characteristic in which probability is weighed is exclusively dependent on a current state and a passage of time. In other words, past states do not influence the Markov Chain, only the current state. The transition from the current state to the next state in a <b>Markov Chain</b> is determined by using probabilty. The algorithm will consider the probability of a current state transitioning to a potential state and transition based on frequency. \n",
    "\n",
    "---\n",
    "\n",
    "Here is an example to explain the behavior described above. Suppose we have a state machine consisting of two states: State <b>q0</b> represents our initial state. Anything traveling to this state will produce a binary value of <b>1</b>. State <b>q1</b> represents the second state which will produce binary <b>0</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603b38eb",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "\n",
    "\n",
    "<img src=\"imgs/two_state_machine.png\"/>\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e9f613",
   "metadata": {},
   "source": [
    "Let us assume the probability in which state <b>q0</b> will transition to <b>q1</b> is 50/50, vice versa.<br><br> \n",
    "\n",
    "```\n",
    "    P(q0|q0) = 0.50\n",
    "    P(q0|q1) = 0.50\n",
    "    P(q1|q0) = 0.50\n",
    "    P(q1|q1) = 0.50\n",
    "```\n",
    "\n",
    "The above probabilities can be read as follows:  \n",
    "\n",
    "```\n",
    "For P(q0|q0), this describes the probability that a transition from state q0 -> q0 will have a frequency of 50%.  \n",
    "\n",
    "For P(q0|q1), this describes the probability that a transition from state q0 -> q1 will have a frequency of 50%.  \n",
    "\n",
    "For P(q1|q0), this describes the probability that a transition from state q1 -> q0 will have a frequency of 50%.  \n",
    "\n",
    "For P(q1|q1), this describes the probability that a transition from state q1 -> q1 will have a frequency of 50%.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da160055",
   "metadata": {},
   "source": [
    "---\n",
    "In the next example, we will generate a generic state machine with a total of three states, increasing the number of potential transitions to three."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9afa8c",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "\n",
    "\n",
    "<img src=\"imgs/three_state_machine.png\"/>\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cddc89",
   "metadata": {},
   "source": [
    "In this example, we only consider the probabilty of transition from one state to another: information such as the alphabet and grammar are ignored.  \n",
    "\n",
    "The probabilities are listed below:\n",
    "\n",
    "```\n",
    "P(q0|q0) = 0.20\n",
    "P(q0|q1) = 0.40\n",
    "P(q0|q2) = 0.20\n",
    "\n",
    "P(q1|q0) = 0.50\n",
    "P(q1|q1) = 0.25\n",
    "P(q1|q1) = 0.25\n",
    "\n",
    "P(q2|q0) = 0.10\n",
    "P(q2|q1) = 0.80\n",
    "P(q2|q2) = 0.10\n",
    "```\n",
    "\n",
    "So how do we measure probability? We could create a set of all words seen after an n-gram or previous word. Then use the set to keep track of how many words we have\n",
    "seen so far. Use the amount of words total, then use the number of specific words to calculate the probability that each word could be selected. From there, we can\n",
    "generate a number between 0-1, dividing which word will be selected based on the float value between 0-1. This is a viable implementation, but I thought it would bloat the\n",
    "code and introduce many complicated math equations that are confusing and complicated to think about.\n",
    "\n",
    "Here is the solution I came up with:\n",
    "- Add all words after an n-gram to a \"bucket\" regardless of whether is has been seen before.\n",
    "- Select from the bucket at random. Words that exist in the bucket multiple times will have a higher probability due to filling more space in the bucket.\n",
    "\n",
    "Using the above solution, this will also solve the issue of delivering sentences that make sense grammatically. By choosing randomly from a bucket of words that\n",
    "have been seen after a certain n-gram, we can ensure that the word has been used in that context before. This means that if we have a gram that has a final word of\n",
    "`runs`, we should see in the bucket primarily adjectives or adverbs such as `fast`, `quickly`, or `slow`. Basically only words that have been seen in the source text\n",
    "from the Nth + 1 position. The benefit of this implementation is that we don't have to adhere to a specific set of grammar rules: we could generate text from source materials\n",
    "in diffferent languages, or even a made up language!  \n",
    "\n",
    "A potential side effect to this solution is space. For very large text, we end up storing a lot of words. However, I tested the working code with three large text files\n",
    "and the program still produces an output within a second and does not throw a memory error. Further below, I also show how much memory the method uses to produce new text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193963db",
   "metadata": {},
   "source": [
    "## Dataset Used\n",
    "\n",
    "The source texts I used are located in `data/` and `pdfs/`. The .txt files inside `data/` are text versions of each pdf book. The wonderful aspect of using books as\n",
    "the dataset is they provide a plentiful amount of sentences, all from a single author. This means we can produce better results in our sentence generator\n",
    "as we can gather a lot of sentences from the same author, allow us to better fill each bucket in an n-gram, further increasing our likelihood of diverging from the source text.\n",
    "Additionally, we can add more variety in style by introducing different authors but maintaining the genre type. This will add variety to our sentence structure and topic, but keep\n",
    "a certain 'feel' in mind. Or we can combine completely different genres like 50 Shades of Gray and Lord of the Rings for a funny and sometimes disturbing result.  \n",
    "\n",
    "Seeing as there are so much materials that we can pull from, it is really easy to begin producing different sentences in a different style. However, the downside to this dataset are\n",
    "ebooks are typically given in the form of a pdf (this is how I found my dataset). I ended up having to copy and paste the entire document into a text editor. Some pdf files were\n",
    "relatively easy to work with, such as the Hunger Games. The Hunger Games pdf did not include page numbers or chapter titles. However, the Lord of the Rings pdf did include\n",
    "chapter titles, page numbers, and remainder marks which made formatting the text file to exclude those relatively challenging. I ended up having to create a temporary script to\n",
    "modify the text to ignore these items. Additionally, Lord of the Rings had a slightly different design choice with quotations. I had to ensure that the quotations used matched\n",
    "the other files or else they would not produce the same n-gram or sometimes falsely raise a stop word / stop character. Additionally, Lord of the Rings would break words with a\n",
    "`-`, which occassionally shows up in the generated materials. Consistency between each text is very important, so each text provied should go through some filtering process to ensure\n",
    "the format is similar to the other texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad862e27",
   "metadata": {},
   "source": [
    "## Setting up the Code\n",
    "\n",
    "---\n",
    "\n",
    "First we need to initalize a class with all the information we will need\n",
    "for a Markov Chain. Here are a few that we will need:\n",
    "\n",
    "- a list of words from our source text for which to build the chain.\n",
    "- a dictionary to store the n-grams and list of next words.\n",
    "- the name of the source file (in case of accessing later.\n",
    "\n",
    "I also thought it would be cool to hold some information regarding the total number of\n",
    "characters, words, and unique words in the text. We will store this information in a dataclass\n",
    "labeled `TextSpecs`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "53016424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JDC used to extend a Class Object in Jupyter Notebook\n",
    "import jdc\n",
    "\n",
    "\n",
    "import random\n",
    "from colorama import Fore, Style\n",
    "from dataclasses import dataclass\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "HUNGER_GAMES_FILENAME = './data/hunger_games.txt'\n",
    "TWILIGHT = './data/twilight.txt'\n",
    "FIFTY_SHADES_OF_GRAY = './data/50_shades_of_gray.txt'\n",
    "LORD_OF_THE_RINGS = './data/lord_of_the_rings.txt'\n",
    "\n",
    "\n",
    "STOP_CHARACTERS = '.?!'\n",
    "STOP_WORDS = ['Dr.', 'Jr.', 'Sr.', 'Mr.', 'Mrs.', 'Ms.', 'Miss.', 'Prof.']\n",
    "FULL_QUOTE = '\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c0ce22",
   "metadata": {},
   "source": [
    "# TextSpecs DataClass\n",
    "\n",
    "---\n",
    "\n",
    "I wanted to create a way to look at how many characters, words, and unique words existed in the text(s) we are using for the program.\n",
    "Below is that method, a simple `dataclass` to hold num_chars, num_words, and num_unique_words. Additionally, there are two functions,\n",
    "`TextSpecs._populate()` helps add more chars, words, and unique words to the existing lvalues, whereas `TextSpecs.display_specs()` displays the current specs\n",
    "of the three variables to the terminal in a clean way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c64ae4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TextSpecs:\n",
    "    num_chars: int = 0\n",
    "    num_words: int = 0\n",
    "    num_unique_words: int = 0\n",
    "        \n",
    "        \n",
    "    def _populate(self, num_chars: int, num_words: int, num_unique_words: int):\n",
    "        self.num_chars += num_chars\n",
    "        self.num_words += num_words\n",
    "        self.num_unique_words += num_unique_words\n",
    "        \n",
    "    \n",
    "    def reset_specs(self):\n",
    "        self.num_chars = 0\n",
    "        self.num_words = 0\n",
    "        self.num_unique_words = 0\n",
    "\n",
    "\n",
    "    def display_specs(self):\n",
    "        print(f'{Style.BRIGHT}{Fore.LIGHTGREEN_EX}{\"#\" * 18}' \\\n",
    "              f'{\"#\" * (len(str(self.num_unique_words)) + 1)}{Style.RESET_ALL}')\n",
    "        \n",
    "        print(f'{Style.BRIGHT}num chars: {Style.RESET_ALL}{self.num_chars}{Style.RESET_ALL} ')\n",
    "        print(f'{Style.BRIGHT}num words: {Style.RESET_ALL}{self.num_words}{Style.RESET_ALL} ')\n",
    "        print(f'{Style.BRIGHT}num unique words: {Style.RESET_ALL}{self.num_unique_words}{Style.RESET_ALL} ')\n",
    "        \n",
    "        print(f'{Style.BRIGHT}{Fore.LIGHTGREEN_EX}{\"#\" * 18}' \\\n",
    "              f'{\"#\" * (len(str(self.num_unique_words)) + 1)}{Style.RESET_ALL}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3cda0b",
   "metadata": {},
   "source": [
    "# MarkovChain Class\n",
    "\n",
    "---\n",
    "\n",
    "Below is the initializer for the MarkovChain. It inherits from `TextSpecs` as that will help keep track of the statistics\n",
    "regarding the total number of words, characters, and unique words we are using for the chain. It is important to note\n",
    "that `TextSpecs._populate()` keeps the original values and adds to it using the augmented assignment operator. This\n",
    "means we should not call these functions directly in practice, but should use the wrapper function defined further\n",
    "down the page. I denoted this by using an underscore before the functions that should not be called alone.\n",
    "<br><br>\n",
    "Markov Chain will be our base class for generating a sentence. It will construct the n_grams and starting n grams, collect the stop\n",
    "words, stop characters, and filenames to use, and collect the size N for the n_gram. It inherits from TextSpecs,\n",
    "where TextSpecs keeps track of the number of characters, words, and unique words for all the files provided to the\n",
    "constructor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8c073f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovChain(TextSpecs):\n",
    "\n",
    "\n",
    "    def __init__(self, filenames, N, stop_characters=None, stop_words=None):\n",
    "        self.initial_words = []\n",
    "        self.n_grams = {}\n",
    "        self.starting_n_grams = []\n",
    "        self.filenames = filenames\n",
    "        self.stop_characters = stop_characters\n",
    "        self.stop_words = stop_words\n",
    "        self.N = N\n",
    "\n",
    "\n",
    "    def display_specs(self):\n",
    "            print(f'{Style.BRIGHT}Files:{Style.RESET_ALL}')\n",
    "            for filename in self.filenames:\n",
    "                print(f'{Style.BRIGHT}{Fore.LIGHTRED_EX}-{Style.RESET_ALL} {filename}')\n",
    "            super().display_specs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113cd2bf",
   "metadata": {},
   "source": [
    "\n",
    "    For the methods below, these will be wrapped with a function to keep the proper\n",
    "    states of the initialized variables within `MarkovChain` and `TextSpecs`\n",
    "\n",
    "## MarkovChain._init_words()\n",
    "\n",
    "This method will iterate over each file provided as source text, first collecting data such as the number of characters, words, and unique words in the text file\n",
    "and populating it to `TextSpecs`. Then, it extends the `initial_words` with the collection of words in the source text. We can atleast verify the number of characters are correct by performing a `wc` command on the source text. \n",
    "\n",
    "Here is the output from that:\n",
    "\n",
    "```\n",
    "wc data/hunger_games.txt\n",
    "    9724  299960 1652553 data/hunger_games.txt\n",
    "```\n",
    "\n",
    "To first collect the number of chars and words using Python, we can take advantage of the .read() and .split() methods to divide the source text into chars and words.\n",
    "Then we just need to use the len() function to gather the length of the list of chars, list of words, and the set of words. The set() does not allow repeat elements in\n",
    "the data structure, so applying this to the `words` list will supply only unique elements. Here is a sample of the functionality from the interpreter:\n",
    "\n",
    "```\n",
    ">>> chars = f.read()\n",
    ">>> chars\n",
    "['H','e','l','l','o',',',' ','W', ... ]\n",
    ">>> len(chars)\n",
    "18\n",
    ">>> words = chars.split() # By default split at the space\n",
    ">>> words\n",
    "['Hello,', 'World!', 'Hello,']\n",
    ">>> len(words)\n",
    "3\n",
    ">>> unique_words = set(words)\n",
    ">>> unique_words\n",
    "['Hello,', 'World!']\n",
    ">>> len(unique_words)\n",
    "2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4d0f9962",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to MarkovChain\n",
    "\n",
    "def _init_words(self):\n",
    "    \n",
    "    for filename in self.filenames:\n",
    "        with open(filename, 'r') as f:\n",
    "            chars = f.read()\n",
    "            words = chars.split()\n",
    "            unique_words = set(words)\n",
    "            self._populate(len(chars), len(words), len(unique_words))\n",
    "            self.initial_words.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b066d0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mFiles:\u001b[0m\n",
      "\u001b[1m\u001b[91m-\u001b[0m ./data/hunger_games.txt\n",
      "\u001b[1m\u001b[92m########################\u001b[0m\n",
      "\u001b[1mnum chars: \u001b[0m1607759\u001b[0m \n",
      "\u001b[1mnum words: \u001b[0m299960\u001b[0m \n",
      "\u001b[1mnum unique words: \u001b[0m26002\u001b[0m \n",
      "\u001b[1m\u001b[92m########################\u001b[0m\n",
      "\n",
      "\u001b[1mPreview of the Text:\u001b[0m\n",
      "When I wake up, the other side of the bed is cold. My fingers stretch out, seeking Prim’s warmth but finding only the rough canvas cover of the mattress. She must have had bad dreams and climbed in with our mother. Of course, she did. This is the day of "
     ]
    }
   ],
   "source": [
    "mc = MarkovChain(filenames=[HUNGER_GAMES_FILENAME],\n",
    "                 N=3,\n",
    "                 stop_characters=STOP_CHARACTERS,\n",
    "                 stop_words=STOP_WORDS\n",
    "                )\n",
    "\n",
    "mc._init_words()\n",
    "mc.display_specs()\n",
    "\n",
    "print(f'\\n{Style.BRIGHT}Preview of the Text:{Style.RESET_ALL}')\n",
    "for i in range(50):\n",
    "    print(mc.initial_words[i], end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5121a3",
   "metadata": {},
   "source": [
    "## MarkovChain._create_ngram_dict()\n",
    "\n",
    "This is where the probability between states comes in to play. Note here, when we add the next word beyond the\n",
    "n-gram (the Nth + 1 word), we allow duplicates into the list. This means we could recieve a list such as \n",
    "`[the, the, the, tiny]` where 75% of the words are `the` and 25% are `tiny`. When selecting from this list\n",
    "in the future, this means that if we select from the bucket randomly, we should see a selection of the\n",
    "word `the` approximately 75% of the time.  \n",
    "\n",
    "As for creating the dictionary of ngrams, the algorithm is fairly straightforward. Suppose we have the sentence below:\n",
    "\n",
    "`The dog is happy. The dog is quite a good boy. That is quite a smile you have there.`\n",
    "\n",
    "The algorithm iterates over a group of N + 1 words at a time:\n",
    "\n",
    "- Itr 1: `The dog is happy.`\n",
    "- Itr 2: `dog is happy. The`\n",
    "- Itr 3: `is happy. The dog`\n",
    "- . . .\n",
    "\n",
    "For each iteration, the `key` for the dictionary becomes the first N elements of the partial list. Then, at the given key,\n",
    "we append the Nth + 1 word beyond it to the bucket. For example:\n",
    "\n",
    "- <b>Itr 1:</b><br>\n",
    "    key: `The dog is`<br> bucket: `happy`\n",
    "- <b>Itr 2:</b><br>\n",
    "    key: `dog is happy.`<br> bucket: `The`\n",
    "- <b>Itr 3:</b><br>\n",
    "    key: `is happy. The`<br> bucket: `dog`\n",
    "- <b>. . .</b>\n",
    "- <b>Itr 5:</b><br>\n",
    "    key: `The dog is`<br> bucket: `happy, quite`\n",
    "\n",
    "Iteration 1 and 5 end up producing the same key for the dictionary, so `quite` is appended to the list from iteration 1. This collision will\n",
    "also occur (with a N=3) from the phrase `is quite a` as that combination of words exists twice in the example.\n",
    "\n",
    "key: `is quite a`<br>\n",
    "bucket: `good, smile`<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "97227460",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to MarkovChain\n",
    "\n",
    "def _create_ngram_dict(self):\n",
    "    n_grams = zip(*[self.initial_words[i:] for i in range(self.N + 1)])\n",
    "    for n_gram in n_grams:\n",
    "        key = n_gram[:self.N]\n",
    "        next_word = n_gram[-1]\n",
    "        self.n_grams[key] = self.n_grams.get(key, []) + [next_word]\n",
    "        \n",
    "        \n",
    "def _create_starting_ngram_list(self):\n",
    "    \n",
    "    is_valid      = lambda g: g[0] not in self.stop_words and (g[1][0].isupper() or g[1][0] in [\"'\", '\"'])\n",
    "    in_stop_chars = lambda g: g[0][-1] in self.stop_characters\n",
    "    \n",
    "    n_grams = zip(*[self.initial_words[i:] for i in range(self.N + 1)])\n",
    "    for n_gram in n_grams:\n",
    "        if in_stop_chars(n_gram) and is_valid(n_gram):\n",
    "            self.starting_n_grams.append(n_gram[1:])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416a7d12",
   "metadata": {},
   "source": [
    "## Preview Results from the N-Grams and the Entries\n",
    "\n",
    "Here is an example of the output produced from `MarkovChain._create_ngram_dict()`. We can see the output shifts one words to the right\n",
    "in the input stream (source text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fe3c3c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mPreview of N-Grams:\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('When', 'I', 'wake')\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('I', 'wake', 'up,')\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('wake', 'up,', 'the')\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('up,', 'the', 'other')\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('the', 'other', 'side')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mc.n_grams = {} # Only used because we are calling this multiple times.\n",
    "\n",
    "mc._create_ngram_dict()\n",
    "n_gram_vals = list(mc.n_grams.values())\n",
    "\n",
    "print(f'\\n{Style.BRIGHT}Preview of N-Grams:{Style.RESET_ALL}')\n",
    "for i, key in enumerate(mc.n_grams.keys()):\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(f'{Style.BRIGHT}- {Style.RESET_ALL}{Fore.LIGHTGREEN_EX}{key}{Style.RESET_ALL}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d284bb9",
   "metadata": {},
   "source": [
    "Additionally we have the buckets or entries that were assigned to each key. The first five elements in the preview of the n-gram entries correspond\n",
    "to the first five keys in the above preview. The remaining five entries are just other buckets that have more than two items inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bb3a93f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mPreview of the N-Gram Entries:\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92mup,\u001b[0m \u001b[92mup,\u001b[0m \n",
      "\u001b[1m- \u001b[0m\u001b[92mthe\u001b[0m \u001b[92mI’m\u001b[0m \n",
      "\u001b[1m- \u001b[0m\u001b[92mother\u001b[0m \u001b[92mrestraints\u001b[0m \n",
      "\u001b[1m- \u001b[0m\u001b[92mside\u001b[0m \n",
      "\u001b[1m- \u001b[0m\u001b[92mof\u001b[0m \u001b[92mof\u001b[0m \u001b[92mof\u001b[0m \u001b[92mof\u001b[0m \n",
      "\u001b[1m- \u001b[0m\u001b[92mof\u001b[0m \u001b[92mof\u001b[0m \u001b[92mof\u001b[0m \u001b[92mof\u001b[0m \n",
      "\u001b[1m- \u001b[0m\u001b[92mthe\u001b[0m \u001b[92mthe\u001b[0m \u001b[92mthe\u001b[0m \u001b[92mher\u001b[0m \n",
      "\u001b[1m- \u001b[0m\u001b[92mbed\u001b[0m \u001b[92mbargain\u001b[0m \u001b[92mtable.\u001b[0m \u001b[92mbuilding\u001b[0m \u001b[92mdome,\u001b[0m \u001b[92mbag\u001b[0m \u001b[92mlake\u001b[0m \u001b[92mSeam.\u001b[0m \u001b[92mcircle?”\u001b[0m \u001b[92mCornucopia,\u001b[0m \u001b[92mnarrow\u001b[0m \u001b[92mbed,\u001b[0m \u001b[92mfamily\u001b[0m \u001b[92mother\u001b[0m \u001b[92mbed\u001b[0m \u001b[92mtree,\u001b[0m \u001b[92mnet.”\u001b[0m \u001b[92mwarehouse.\u001b[0m \u001b[92mV,\u001b[0m \u001b[92mbed,\u001b[0m \u001b[92mtunnel,\u001b[0m \u001b[92mbridge\u001b[0m \u001b[92mhead\u001b[0m \u001b[92mbargain,\u001b[0m \u001b[92mbalcony\u001b[0m \u001b[92mhouse,\u001b[0m \u001b[92mhouse.”\u001b[0m \n",
      "\u001b[1m- \u001b[0m\u001b[92mis\u001b[0m \u001b[92mis\u001b[0m \u001b[92mand\u001b[0m \n",
      "\u001b[1m- \u001b[0m\u001b[92mhad\u001b[0m \u001b[92mreally\u001b[0m \u001b[92mbeen\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "print(f'\\n{Style.BRIGHT}Preview of the N-Gram Entries:{Style.RESET_ALL}')\n",
    "for n_gram in n_gram_vals[:5]:\n",
    "    print(f'{Style.BRIGHT}- {Style.RESET_ALL}', end='')\n",
    "    for gram in n_gram:\n",
    "        print(f'{Fore.LIGHTGREEN_EX}{gram}{Style.RESET_ALL}', end=' ')\n",
    "    print()\n",
    "    \n",
    "n_gram_new = list(filter(lambda x: len(x) > 2, n_gram_vals))\n",
    "for entries in n_gram_new[:5]:\n",
    "    print(f'{Style.BRIGHT}- {Style.RESET_ALL}', end='')\n",
    "    for entry in entries:\n",
    "        print(f'{Fore.LIGHTGREEN_EX}{entry}{Style.RESET_ALL}', end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ce6598b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mPreview of the N-Gram Starters:\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('My', 'fingers', 'stretch')\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('She', 'must', 'have')\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('Of', 'course,', 'she')\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('This', 'is', 'the')\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('I', 'prop', 'myself')\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('There’s', 'enough', 'light')\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('My', 'little', 'sister,')\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('In', 'sleep,', 'my')\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('Prim’s', 'face', 'is')\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('My', 'mother', 'was')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mc.starting_n_grams = [] # Only used because we are calling this multiple times.\n",
    "\n",
    "mc._create_starting_ngram_list()\n",
    "print(f'\\n{Style.BRIGHT}Preview of the N-Gram Starters:{Style.RESET_ALL}')\n",
    "for n_gram in mc.starting_n_grams[:10]:\n",
    "    print(f'{Style.BRIGHT}- {Style.RESET_ALL}{Fore.LIGHTGREEN_EX}{n_gram}{Style.RESET_ALL}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77d5f40",
   "metadata": {},
   "source": [
    "An issue that I brought up in the introduction referred to memory usage of the program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d37f8e6",
   "metadata": {},
   "source": [
    "Although the total memory for the `MarkovChain` class is only around 1/100th of a gigabyte of memory, since Python does not limit the program memory,\n",
    "this is well within the bounds of running for computers. \n",
    "Still, for generating new sentences this seems quite wasteful. Although this implementation is nice to work with, the side effect is we are storing\n",
    "many duplicates of the same word, which add up in a hurry for large source texts. Additionally, the SentenceGenerator will require a lot of source text\n",
    "in order to produce consitently new sentences. So, in order to provide a decent sentence generator, we will have to sacrifice memory for this implementation. \n",
    "\n",
    "A potential solution to optimize this could be to, instead of storing\n",
    "the same word in a bucket multiple times, have an integer value to represent the likelihood of a word being chosen out of the group of potential words.\n",
    "Using this method, lets say we have an example like this:  \n",
    "\n",
    "```\n",
    "key: 'The next word'\n",
    "bucket: ['is', 'is','is','is','is','is','is','is','is','is','is','is','is','is','is', ...]\n",
    "```\n",
    "\n",
    "If we have the same word that is repeated many times, it could be more efficient to simply show that if we see the key 'the next word' that any number between\n",
    "0 and 1 we choose, the result will 100% be `is`. Because of this, we don't need to store all strings `is`, but we could simply replace the the `is` with a number\n",
    "that signifies how many instances of that word exist in the bucket. If we have 100 words that are `is` and 25 that are `found`, then we can perform the following calculation:\n",
    "\n",
    "```\n",
    "key: is, num: 100  \n",
    "key: found: num: 25\n",
    "total = sum(keys) = 125\n",
    "Likelihood of 'is': 100 / 125 = 0.8\n",
    "Likelihood of 'found': 25 / 125 = 0.2\n",
    "\n",
    "Select a number randomly between 0-1. If <num> <= 0.2, then select 'found', otherwise select 'is'.\n",
    "```\n",
    "\n",
    "By storing how many of the same word exists in a bucket, we can reduce a series of identical strings to a single integer. This would be ideal for extremely common words\n",
    "that follow a specific gram, but on the other end, this implementation could produce little effect for the below scenario:\n",
    "\n",
    "```\n",
    "n-gram key: ('The dog runs')\n",
    "n-gram bucket: ['wild', 'free', 'blindly', excitedly', 'willingly', 'dangerously', ... ]\n",
    "```\n",
    "\n",
    "In this example we have a statement: `The dog runs`. The words beyond `runs` that are in the bucket represent decorations for the sentence, or adverbs. There could be\n",
    "an extremely large amount of different adverbs used after a specific n-gram. This places the new solution in the same situation as before: too many unique words to store\n",
    "in a bucket will take up memory. I do not see a potential solution for this specific problem, as if there is a unique item, we have to store it in memory somehow. Otherwise,\n",
    "we would not be able to select said solution / word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4a2603",
   "metadata": {},
   "source": [
    "## Limitations to MarkovChain\n",
    "\n",
    "There is one aspect of the `MarkovChain` implementation that I am not proud of. The contents inside `MarkovChain.n_grams` include keys with capitalizations, which means there could be times where\n",
    "the keys match, however one key started a sentence and another was in the middle of the sentence. This limits our options of what words could be next. Since this approach already\n",
    "needs a lot of material to work effectively, this is just further limiting the variety of sentences it will be able to generate. A potential improvement and solution to this problem\n",
    "would be to make every letter in the key lower case after initializing the `MarkovChain.starting_n_grams`. This way we can maximize the potential choices for each gram. However, the\n",
    "side effect to ths approach means that we will eventually have to determine where a sentence starts and where a sentence ends. Additionally, I don't see a good way to do the same\n",
    "thing for punctuations: if we remove a punctuation, there will be no way to reasonably determine when a sentence can end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd9a055",
   "metadata": {},
   "source": [
    "# SentenceGenerator Class\n",
    "\n",
    "---\n",
    "\n",
    "Now that we have a skeleton for generating sentences, we will inherit from the `MarkovChain` to use its functionality to generate\n",
    "sentences. The purpose of this new class is to simply extend the behavior of the `MarkovChain` class. Therefore, the `SentenceGenerator.__init__()`\n",
    "method will only call the constructor of the base class and nothing more. For this report, I also imported the complete `MarkovChain` class\n",
    "for `SentenceGenerator` to inherit from as Jupyter-Notebook seemed to have some inconsistencies with inheritance when also using the `jdc` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e7db70e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MarkovChain.MarkovChain import MarkovChain\n",
    "\n",
    "class SentenceGenerator(MarkovChain):\n",
    "\n",
    "    def __init__(self, filenames, N, stop_characters=None, stop_words=None):\n",
    "        super().__init__(filenames, N, stop_characters=stop_characters, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b96401",
   "metadata": {},
   "source": [
    "## SentenceGenerator.generate_sentence [1st iteration]\n",
    "\n",
    "This is the initial draft of the generate_sentence method. It it actually generates\n",
    "understandable text and was my first solution that showed a promising output return.\n",
    "The key to making this function work was to separate the n-grams with n-grams that can\n",
    "start a sentence. These are `self.n_grams` and `self.starting_n_grams`, respectively.\n",
    "\n",
    "By intializing with an n-gram that is the beginning of a sentence,\n",
    "We can start the chain from the beginning of the sentence instead of midway through or at the end.\n",
    "This solution shows another issue: the sentences that it has generated, although somewhat coherent,\n",
    "end midway through a sentence. This issue took a lot of time to find a decent solution to. The `SentenceGenerator.generate_sentence()`\n",
    "method described below is fairly condensed and works well for beginning a sentence. Additionally,\n",
    "being able to provide a variety of length for a generated sentence is a great way to increase the variety of\n",
    "sentences to be generated. The issue is this approach does not consider how a sentence ends. It could end,\n",
    "if the length of the sentence determine conveniently lands on an ending phrase. However, most of the time,\n",
    "this does not occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4fa210f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to SentenceGenerator\n",
    "\n",
    "def generate_sentence(self):\n",
    "    \n",
    "    length_sentence = random.randint(4, 15)  \n",
    "    seed = random.choice(self.starting_n_grams)\n",
    "    output = [x for x in seed]\n",
    "    for _ in range(length_sentence):\n",
    "        word = random.choice(self.n_grams[seed])\n",
    "        seed = tuple(list(seed[1:]) + [word])\n",
    "        output.append(word)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ef916dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPreview of Generated Sentences:\u001b[0m\n",
      "\u001b[1m\u001b[31m- \u001b[0mRay hands me a glass of water.” Her voice is stern. Olivia scoots up immediately and \n",
      "\u001b[1m\u001b[31m- \u001b[0mHow much I love him. “Ana, honey.” The voice \n",
      "\u001b[1m\u001b[31m- \u001b[0mBut so it is, Sam: in that land you \n",
      "\u001b[1m\u001b[31m- \u001b[0m\"Heel, Wolf!\" To the relief of Frodo and Sam, the dogs walked away and let them ride \n",
      "\u001b[1m\u001b[31m- \u001b[0mRight now, it can provide neither, at least at the standard the people are accustomed \n"
     ]
    }
   ],
   "source": [
    "# Initializing SentenceGenerator Object\n",
    "sg = SentenceGenerator(filenames=[\n",
    "                                HUNGER_GAMES_FILENAME,\n",
    "                                TWILIGHT,\n",
    "                                FIFTY_SHADES_OF_GRAY,\n",
    "                                LORD_OF_THE_RINGS\n",
    "                                ],\n",
    "                    N=3,\n",
    "                    stop_characters=STOP_CHARACTERS,\n",
    "                    stop_words=STOP_WORDS,\n",
    "                    )\n",
    "\n",
    "print(f'{Style.BRIGHT}Preview of Generated Sentences:{Style.RESET_ALL}')\n",
    "for _ in range(5):\n",
    "    print(f'{Style.BRIGHT}{Fore.RED}- {Style.RESET_ALL}', end='')\n",
    "    for word in sg.generate_sentence():\n",
    "        print(word, end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1702b290",
   "metadata": {},
   "source": [
    "## SentenceGenerator.generate_sentence [2nd iteration]\n",
    "\n",
    "In this second iteration, I address the issue where the sentence ends halfway through. Additionally,\n",
    "a few times in the iteration one implementation, there is sometimes a random quotation that tries to\n",
    "start a quote or end a quote. This will also be addressed in this iteration. Testing for a quote at\n",
    "the beginning or end is also quite a difficult problem to fix in a simple way. Somehow, we have to keep\n",
    "track of the current state the generator is in (does it need to search for an ending quote, has it seen\n",
    "a closing quote but no opening quote?). The second problem is more challenging. It is pretty straightforward\n",
    "to search for a closing quote after seeing an opening, but what are we to do if we see a closing quote?\n",
    "\n",
    "\n",
    "To be honest, I don't have a good solution to this issue. One \"solution\" would be to eliminate the quotes\n",
    "all-together from the generator, but that is no fun. The next solution could be to insert the starting quote\n",
    "at the start of the previous sentence, but there are too many conditions to consider. For example,\n",
    "\n",
    "```\n",
    "- She said, \"Hello, foo! How is bar?\"\n",
    "- \"Hello, foo!\" she said, \"How is bar?\"\n",
    "- \"Hello, foo! How is bar?\" She said.\n",
    "```\n",
    "\n",
    "In the first example, we can't just insert the start quote at the beginning of the sentence, as that would\n",
    "be incorrect. Additionally, the second condition is even harder, for we have to potentially insert two quotes\n",
    "in one sentence! The last example would be the only time where the \"fix\" would work as intended. The problem\n",
    "with the first two examples are the word \"said\" or \"she\" can be replaced with too many different words such\n",
    "as \"He\", \"Jared\", or \"exlaimed\", \"cried.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6f408518",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to SentenceGenerator\n",
    "\n",
    "def generate_sentence(self, len: int=None):\n",
    "    \n",
    "    length_sentence = random.randint(4, 15) if len is None else len\n",
    "    seed = random.choice(self.starting_n_grams)\n",
    "    \n",
    "    \n",
    "    output = [x for x in seed]\n",
    "    is_quote = reduce(lambda base, word: (word[0] == FULL_QUOTE) or base, output, False)\n",
    "\n",
    "    for _ in range(length_sentence):\n",
    "        word, seed = self._generate_word(seed, is_quote)\n",
    "        output.append(word)\n",
    "        \n",
    "    self._end_sentence(output, seed, is_quote)\n",
    "    return ' '.join(output).rstrip()\n",
    "\n",
    "\n",
    "def _generate_word(self, seed, is_quote=False):\n",
    "    \n",
    "    not_ending_quote = lambda word: word[-1] != FULL_QUOTE\n",
    "    \n",
    "    words = self.n_grams[seed]\n",
    "    words = [word for word in words if not_ending_quote(word)] if not is_quote else words\n",
    "\n",
    "    # This is needed. If the text is primarily quotes it will make empty list.\n",
    "    if not len(words):\n",
    "        words = self.n_grams[seed]\n",
    "\n",
    "    word = random.choice(words)\n",
    "    seed = tuple(list(seed[1:]) + [word])\n",
    "    \n",
    "    return word, seed\n",
    "    \n",
    "\n",
    "def _end_sentence(self, output, seed, is_quote=False):\n",
    "    \n",
    "    in_stop_characters = lambda word: word[-1] in self.stop_characters\n",
    "    in_stop_words      = lambda word: word in self.stop_words\n",
    "    \n",
    "    while not ((end := [word for word in self.n_grams[seed] if in_stop_characters(word) and not in_stop_words(word)]) and not is_quote):\n",
    "        word, seed = self._generate_word(seed, is_quote)\n",
    "        is_quote |= word[0] == FULL_QUOTE  # if quote at beginning, make true.\n",
    "        is_quote &= word[-1] != FULL_QUOTE # if quote at end, make false.\n",
    "        output.append(word)\n",
    "        \n",
    "    word = random.choice(end)\n",
    "    output.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "acf0c5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPreview of Generated Sentences [Iteration 2]:\u001b[0m\n",
      "\u001b[1m\u001b[31m- \u001b[0mTaking my Stanley knife from the back of his head, and spring out of the trees.\n",
      "\u001b[1m\u001b[31m- \u001b[0mWhat do you think I shall have such need of it.\n",
      "\u001b[1m\u001b[31m- \u001b[0mSome buy into it for the benefit of those in the Capitol tomorrow morning.\n",
      "\u001b[1m\u001b[31m- \u001b[0mTaking Rue on as an ally. Extended my hand to him.\n",
      "\u001b[1m\u001b[31m- \u001b[0mThen he stopped, and a shadow came over his dreams and he turned slowly to glare at him, my face is projected in a tight close-up on the screen, but I don’t care.\n"
     ]
    }
   ],
   "source": [
    "print(f'{Style.BRIGHT}Preview of Generated Sentences [Iteration 2]:{Style.RESET_ALL}')\n",
    "for _ in range(5):\n",
    "    sentence = sg.generate_sentence()\n",
    "    print(f'{Style.BRIGHT}{Fore.RED}- {Style.RESET_ALL}{sentence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff4a6b6",
   "metadata": {},
   "source": [
    "This is producing better results. There are still little issues regarding quotations, but it is more consistent than before. Let's now make multiple sentences to produce\n",
    "a paragraph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bf477f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to SentenceGenerator\n",
    "\n",
    "def generate_paragraph(self, len: int=None):\n",
    "\n",
    "    num_sentences = random.randint(5,20)\n",
    "    output = []\n",
    "    for _ in range(num_sentences):\n",
    "        output.append(self.generate_sentence())\n",
    "    return ' '.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cc702511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were the prints of orc-feet in the earth. But soon Haldir turned aside into the trees by the shore, his skin sparkling like the sea. I was thinking we could do with a length of silk. But what a good time to begin to understand, Frodo, after all you may need to touch me for what lies underneath. For the record - you stood beside me knowing what I was hoping. But then I remember that Plutarch is a Head Gamemaker, isn’t it?” Snow dabs the corners of his mouth. Is he your boyfriend?” Whoa... What? “Who?” “The photographer. José Rodriguez.” I laugh, nervous but curious. He’s just sex on legs, and now I have at least one of my hands and I can’t let Peeta do this. Voices hiss. “Get out of here, girl.” “Only make it worse.” “What do you mean? That makes people do all kinds of trouble.” “Me, too,” Gale says. He just manages a smile before the drugs pull him back under. Yet even so, as Ring-bearer and as one that has borne it on finger and seen that which is hidden, your sight is grown keener. \"Go now!\" said Celeborn. \"You are worn with sorrow and much toil. Even if your Quest did not concern us closely, you should have the decency to drop his gaze. Technically, I am unarmed. But no one should look so tempting, it's not fair.\" \"Tempting how?\" I asked. She went back for him!” “Oh, no,” my mother says. We both know I’m right. He touches the button on my jeans, and he leisurely pulls down the zipper. I touch it hesitantly. It’s suede, like a small child. Maybe fear was all he knew then. Sorrow grips and squeezes my shoulder. And she wants to help. This morning I purposely overstuffed my game bag on the floor. They will know all the news he could. Elves, who seldom walked in the door. I tried holding my breath as he pulls spikes of pottery from his palms.\n",
      "\n",
      "Especially now that the sun’s gone down. I wonder what’s going on back home. Miss Steele From: Christian Grey Subject: Your behind Date: May 31 2011 16:10 To: Anastasia Steele Schmules? My head spun around in answerless circles. There was one thing I was conscious of was the sound of him. I instinctively feel the Capitol might want these unpopular Games over as soon as possible. Do you refuse?\" \"At that his breath came like a soft hissing through closed teeth. For here the heirs of Elendil, lest a time come when the memory of an old pine, that hung over a shallow pit: it looked as if it approved of the change in the Games. When we finish, I stare longingly at the dish. \"Promise.\" \"I promise to try to kill himself with starvation.\" \"Is that possible?\" My voice was fervent with gratitude. \"That's twice now.\" His face softened. The rest will follow,” he says. I go over to the opposite post and frees my other leg. Look at this shoe! I sighed, scowling at the blackboard. \"Am I annoying you?\" he asked. He sounded amused. Aware in some corner of my mouth. Slowly he peels it off me and passes me some orange juice. Much more. The surge of jealousy I felt only mo- ments ago tells me that he is – strange indeed though that seems to me. Yes, everyone in the districts are for. To provide the bread and circuses.” “Yes. The slides in the box were out of order. Frodo felt a sudden chill running through him and clutching at his pocket. He smiled. \"I could say the same to the others... as I do now. It’s a thrill to be sitting on the extreme edge of his shirt. my sword.” I can see by the light from below, I can see Peeta’s face beginning to return to it. But today, despite the bright banners hanging on the back of my head, forcing me back to life. How do you remember these things so exactly?” “I see them every night,” he says. “Did you kill her?” “No. Thresh broke her skull with a rock,” I say. “Lucky he didn’t catch you, too,” says Peeta. The energy at the table while Effie rattles her schedule papers and reminds us we’re still on tour. “There’s the Harvest Festival on the final kill? My brain is begin- ning to fog... hmm alcohol. “I’m nervous about the gagging.” “Okay. I’ll take note.” I stare up at him, realization dawning. “Do you like tying your submissives up so they can’t see my face. \"Yes?\" \"You said that Rosalie and Emmett were close behind us, hiding me. I stumbled alongside Edward, still stunned with fear. I couldn't hear the tapping of the rain running down my neck and slowly lower myself until I’m hanging by my hands. For a moment, I hesitate, and I have a vision of Gandalf on one of the aisles to the electrical section. Why is he so worried about my truck. \"Are you going back to class?\" \"Are you kidding? I'd just have to leave our beach soon, anyway. So we break camp, walk over to the archery station. Johanna Mason is naked again and oiling her skin down for a week —\" I saw his mind. But those kids we see on camera waiting for the bolt that signals noon. Why not? No one decent will buy burned bread!” He began to say to Jessica. I sat in silence, staring at the fire.\n",
      "\n",
      "\"Damn it, Bella!\" he broke off, gasping. \"You'll be the death of me, I swear you will.\" I leaned over, bracing my hands against my knees for support. What just went through my mind? There’s no situation in which I don’t get it. Well, I do get to the Cornucopia. They were alive and eagerly welcomed. This enthusiasm was interpreted as kindness. The Head Peacekeeper wipes his hand along the length of it. But from hints dropped among the snarls I gathered that Alice was a bit more composed. \"No,\" he said without opening his eyes. \"What am I saying? I have been expecting Gandalf for many days. He was to have come over him. Ever since the bloodbath, I’ve been featured on screens more than I do for you?” After a thorough examination and lengthy discussion, Dr. Greene and I decide to escape for a while? My mind flits briefly to my mother’s offer. I hit reply. From: Anastasia Steele Subject: Stalker Date: May 27 2011 08:52 To: Christian Grey I have many admirers. And if the audience really thinks we’re in love . . . even if we did . . . this is just what I say,” Beetee assures us. “And where will we be when this happens?” asks Finnick. “Far enough up in the Gamemakers’ faces. They’d have failed the Capitol. Might possibly even be executed, slowly and painfully while the cameras broadcast it to every screen in the country. If Peeta and I visited during the Victory Tour, he challenged me to erase any doubts of my love for Peeta? It was obviously a ploy to distract me and keep me from doing anything else inflammatory in the districts. By the time he looked up and caught his eyes trained on the world outside. “Go to sleep,” he says softly. “I don’t believe you,” says Jackson. “As your current commander, I order you to transfer the prime security clearance to Squad Four-Five-One Soldier Katniss Everdeen.” It’s all he can stand. “Go to bed. You need your rest.” I know I should feel dizzy. The hour seemed very long. I couldn't concentrate on him. I do not like this news,\" he said at last. It’s as though for the last time, I hoped. He walked to the house. The lights inside were bright, but they did little to alleviate the tedium. But there’s been no sign of life from returning to him. Finnick’s hand comes up and pats Venia’s back soothingly, her curvy body looking plumper than usual next to Venia’s thin, angular one. We stopped outside the door to his cell. Hospital room. We had to restrain them after an altercation over some bread,” says the guard.\n",
      "\n",
      "Reading this, I might as well do the watching.\" \"I know what is the peril, the Tale of the Ring, and while it remains they will endure. And the cost?... ” Christian whistles be- tween his teeth. “Sheesh... that was one conversation I could skip. Along the skirts of the Dead Marshes I followed it, and then I shivered. Tying its hands behind its back would be a crime to forget. It’s impossible not to think about what my feelings might be, honestly, because it was breached. Avoiding the mirror, I can see even Caesar’s professionalism showing some cracks as he tries to continue.\n",
      "\n",
      "She's getting something to eat right now.\" \"She's here?\" I tried to refocus. I have the same reaction I did the camouflage thing, like you suggested, Katniss.” He hesitates. He opens his eyes and flexes his hips. “You’ll have to learn to manage my expectations. He props himself up on his outrageous offer... if! Because he never underestimates the cruelty of those we failed to convince in the districts. They could remember and repeat long passages of human speech, so they were still slaves. He was very easy to talk with. He flashed a brilliant smile, looking at me curiously. She’s not in District 13 and those who can generally count on supper and thereby ensure we will never be seen on earth again save in memory.\" Sam went red to the ears and muttered something unintelligible. Both you and Bilbo were doing, so close in his little room,\" said Merry.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(sg.generate_paragraph())\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3714a21f5b22468d5a06a5f8f5b645d0465e1d7e98e037e9d760cd6d4c46ee2f"
  },
  "kernelspec": {
   "display_name": "Cpts-315-Markov-Chain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
