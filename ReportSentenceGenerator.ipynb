{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8275c823",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Utilizing Markov Chains with the N-Gram Method to Suggest New Sentences from a Base Text</h1>\n",
    "\n",
    "<br>\n",
    "\n",
    "<b>Project:</b> Markov Chain and N-Grams  \n",
    "<b>Class:</b> Cpts 315 Washington State University  \n",
    "<b>Description:</b> Final Project  \n",
    "<b>By:</b> Kyle Hurd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9f2ee7",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# Introduction - Markov Chain\n",
    "\n",
    "</center>\n",
    "\n",
    "My motivation for designing a Sentence Generator was first inspired by a concept I read when reading about how the Google Definition `Word Usage Over Time` was implemented.\n",
    "Google provides a feature where when you look up the definition of a word, it provides a chart showing said word's usage over a period of time. I then read about how Google\n",
    "used a method called N-Grams to help determine this. This then led me to the concept where one would be able to predict the usage of a word based on the previous words leading up\n",
    "to it. I was initially going to try and apply this concept onto an infitinite supply of Tweets from Twitter, but I was not given permission by Twitter to use their API. So I thought\n",
    "of a different way to apply this N-Gram concept by reading a source text such as a book or article. From analyzing said article, I could generate new sentences that are similar\n",
    "to the source text.  \n",
    "\n",
    "The task I have laid out is to read text from a source text and be able to generate unique combinations of words based upon the original text. The sentences must be able to diverge\n",
    "from the source text (not be identical) which means we will most likely need a lot of source text, preferably matching a similar genre and writing style. Some of the challenges is\n",
    "determining a concrete and understandable method to deliver words that are connected to a certain n-gram, how to measure probabilty that a connected word should be selected over\n",
    "another, and how to make sure the sentences are grammatically correct.\n",
    "\n",
    "<p align=\"center\">\n",
    "\n",
    "<img src=\"imgs/word_usage_chart.png\"/>\n",
    "\n",
    "</p>\n",
    "\n",
    "A <b>Markov Chain</b> is a model that makes predictions based on a sequence of potential states. It will weigh the probability in which a set of states will be in the sequence and use this information to generate a new sequence. The defining characteristic in which probability is weighed is exclusively dependent on a current state and a passage of time. In other words, past states do not influence the Markov Chain, only the current state. The transition from the current state to the next state in a <b>Markov Chain</b> is determined by using probabilty. The algorithm will consider the probability of a current state transitioning to a potential state and transition based on frequency. \n",
    "\n",
    "---\n",
    "\n",
    "Here is an example to explain the behavior described above. Suppose we have a state machine consisting of two states: State <b>q0</b> represents our initial state. Anything traveling to this state will produce a binary value of <b>1</b>. State <b>q1</b> represents the second state which will produce binary <b>0</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603b38eb",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "\n",
    "\n",
    "<img src=\"imgs/two_state_machine.png\"/>\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e9f613",
   "metadata": {},
   "source": [
    "Let us assume the probability in which state <b>q0</b> will transition to <b>q1</b> is 50/50, vice versa.<br><br> \n",
    "\n",
    "```\n",
    "    P(q0|q0) = 0.50\n",
    "    P(q0|q1) = 0.50\n",
    "    P(q1|q0) = 0.50\n",
    "    P(q1|q1) = 0.50\n",
    "```\n",
    "\n",
    "The above probabilities can be read as follows:  \n",
    "\n",
    "```\n",
    "For P(q0|q0), this describes the probability that a transition from state q0 -> q0 will have a frequency of 50%.  \n",
    "\n",
    "For P(q0|q1), this describes the probability that a transition from state q0 -> q1 will have a frequency of 50%.  \n",
    "\n",
    "For P(q1|q0), this describes the probability that a transition from state q1 -> q0 will have a frequency of 50%.  \n",
    "\n",
    "For P(q1|q1), this describes the probability that a transition from state q1 -> q1 will have a frequency of 50%.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da160055",
   "metadata": {},
   "source": [
    "---\n",
    "In the next example, we will generate a generic state machine with a total of three states, increasing the number of potential transitions to three."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9afa8c",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "\n",
    "\n",
    "<img src=\"imgs/three_state_machine.png\"/>\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cddc89",
   "metadata": {},
   "source": [
    "In this example, we only consider the probabilty of transition from one state to another: information such as the alphabet and grammar are ignored.  \n",
    "\n",
    "The probabilities are listed below:\n",
    "\n",
    "```\n",
    "P(q0|q0) = 0.20\n",
    "P(q0|q1) = 0.40\n",
    "P(q0|q2) = 0.20\n",
    "\n",
    "P(q1|q0) = 0.50\n",
    "P(q1|q1) = 0.25\n",
    "P(q1|q1) = 0.25\n",
    "\n",
    "P(q2|q0) = 0.10\n",
    "P(q2|q1) = 0.80\n",
    "P(q2|q2) = 0.10\n",
    "```\n",
    "\n",
    "So how do we measure probability? We could create a set of all words seen after an n-gram or previous word. Then use the set to keep track of how many words we have\n",
    "seen so far. Use the amount of words total, then use the number of specific words to calculate the probability that each word could be selected. From there, we can\n",
    "generate a number between 0-1, dividing which word will be selected based on the float value between 0-1. This is a viable implementation, but I thought it would bloat the\n",
    "code and introduce many complicated math equations that are confusing and complicated to think about.\n",
    "\n",
    "Here is the solution I came up with:\n",
    "- Add all words after an n-gram to a \"bucket\" regardless of whether is has been seen before.\n",
    "- Select from the bucket at random. Words that exist in the bucket multiple times will have a higher probability due to filling more space in the bucket.\n",
    "\n",
    "A potential side effect to this solution is space. For very large text, we end up storing a lot of words. However, I tested the working code with three large text files\n",
    "and the program still produces an output within a second and does not throw a memory error. Further below, I also show how much memory the method uses to produce new text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad862e27",
   "metadata": {},
   "source": [
    "## Setting up the Code\n",
    "\n",
    "---\n",
    "\n",
    "First we need to initalize a class with all the information we will need\n",
    "for a Markov Chain. Here are a few that we will need:\n",
    "\n",
    "- a list of words from our source text for which to build the chain.\n",
    "- a dictionary to store the n-grams and list of next words.\n",
    "- the name of the source file (in case of accessing later.\n",
    "\n",
    "I also thought it would be cool to hold some information regarding the total number of\n",
    "characters, words, and unique words in the text. We will store this information in a dataclass\n",
    "labeled `TextSpecs`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "53016424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JDC used to extend a Class Object in Jupyter Notebook\n",
    "import jdc\n",
    "\n",
    "\n",
    "import random\n",
    "from colorama import Fore, Style\n",
    "from dataclasses import dataclass\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "HUNGER_GAMES_FILENAME = './data/hunger_games.txt'\n",
    "TWILIGHT = './data/twilight.txt'\n",
    "FIFTY_SHADES_OF_GRAY = './data/50_shades_of_gray.txt'\n",
    "LORD_OF_THE_RINGS = './data/lord_of_the_rings.txt'\n",
    "\n",
    "\n",
    "STOP_CHARACTERS = '.?!'\n",
    "STOP_WORDS = ['Dr.', 'Jr.', 'Sr.', 'Mr.', 'Mrs.', 'Ms.', 'Miss.', 'Prof.']\n",
    "FULL_QUOTE = '\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c0ce22",
   "metadata": {},
   "source": [
    "# TextSpecs DataClass\n",
    "\n",
    "---\n",
    "\n",
    "I wanted to create a way to look at how many characters, words, and unique words existed in the text(s) we are using for the program.\n",
    "Below is that method, a simple `dataclass` to hold num_chars, num_words, and num_unique_words. Additionally, there are two functions,\n",
    "`TextSpecs._populate()` helps add more chars, words, and unique words to the existing lvalues, whereas `TextSpecs.display_specs()` displays the current specs\n",
    "of the three variables to the terminal in a clean way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c64ae4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TextSpecs:\n",
    "    num_chars: int = 0\n",
    "    num_words: int = 0\n",
    "    num_unique_words: int = 0\n",
    "        \n",
    "        \n",
    "    def _populate(self, num_chars: int, num_words: int, num_unique_words: int):\n",
    "        self.num_chars += num_chars\n",
    "        self.num_words += num_words\n",
    "        self.num_unique_words += num_unique_words\n",
    "        \n",
    "    \n",
    "    def reset_specs(self):\n",
    "        self.num_chars = 0\n",
    "        self.num_words = 0\n",
    "        self.num_unique_words = 0\n",
    "\n",
    "\n",
    "    def display_specs(self):\n",
    "        print(f'{Style.BRIGHT}{Fore.LIGHTGREEN_EX}{\"#\" * 18}' \\\n",
    "              f'{\"#\" * (len(str(self.num_unique_words)) + 1)}{Style.RESET_ALL}')\n",
    "        \n",
    "        print(f'{Style.BRIGHT}num chars: {Style.RESET_ALL}{self.num_chars}{Style.RESET_ALL} ')\n",
    "        print(f'{Style.BRIGHT}num words: {Style.RESET_ALL}{self.num_words}{Style.RESET_ALL} ')\n",
    "        print(f'{Style.BRIGHT}num unique words: {Style.RESET_ALL}{self.num_unique_words}{Style.RESET_ALL} ')\n",
    "        \n",
    "        print(f'{Style.BRIGHT}{Fore.LIGHTGREEN_EX}{\"#\" * 18}' \\\n",
    "              f'{\"#\" * (len(str(self.num_unique_words)) + 1)}{Style.RESET_ALL}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3cda0b",
   "metadata": {},
   "source": [
    "# MarkovChain Class\n",
    "\n",
    "---\n",
    "\n",
    "Below is the initializer for the MarkovChain. It inherits from `TextSpecs` as that will help keep track of the statistics\n",
    "regarding the total number of words, characters, and unique words we are using for the chain. It is important to note\n",
    "that `TextSpecs._populate()` keeps the original values and adds to it using the augmented assignment operator. This\n",
    "means we should not call these functions directly in practice, but should use the wrapper function defined further\n",
    "down the page. I denoted this by using an underscore before the functions that should not be called alone.\n",
    "<br><br>\n",
    "Markov Chain will be our base class for generating a sentence. It will construct the n_grams and starting n grams, collect the stop\n",
    "words, stop characters, and filenames to use, and collect the size N for the n_gram. It inherits from TextSpecs,\n",
    "where TextSpecs keeps track of the number of characters, words, and unique words for all the files provided to the\n",
    "constructor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8c073f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovChain(TextSpecs):\n",
    "\n",
    "\n",
    "    def __init__(self, filenames, N, stop_characters=None, stop_words=None):\n",
    "        self.initial_words = []\n",
    "        self.n_grams = {}\n",
    "        self.starting_n_grams = []\n",
    "        self.filenames = filenames\n",
    "        self.stop_characters = stop_characters\n",
    "        self.stop_words = stop_words\n",
    "        self.N = N\n",
    "\n",
    "\n",
    "    def display_specs(self):\n",
    "            print(f'{Style.BRIGHT}Files:{Style.RESET_ALL}')\n",
    "            for filename in self.filenames:\n",
    "                print(f'{Style.BRIGHT}{Fore.LIGHTRED_EX}-{Style.RESET_ALL} {filename}')\n",
    "            super().display_specs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113cd2bf",
   "metadata": {},
   "source": [
    "\n",
    "    For the methods below, these will be wrapped with a function to keep the proper\n",
    "    states of the initialized variables within `MarkovChain` and `TextSpecs`\n",
    "\n",
    "## MarkovChain._init_words()\n",
    "\n",
    "This method will iterate over each file provided as source text, first collecting data such as the number of characters, words, and unique words in the text file\n",
    "and populating it to `TextSpecs`. Then, it extends the `initial_words` with the collection of words in the source text. We can atleast verify the number of characters are correct by performing a `wc` command on the source text. \n",
    "\n",
    "Here is the output from that:\n",
    "\n",
    "```\n",
    "wc data/hunger_games.txt\n",
    "    9724  299960 1652553 data/hunger_games.txt\n",
    "```\n",
    "\n",
    "To first collect the number of chars and words using Python, we can take advantage of the .read() and .split() methods to divide the source text into chars and words.\n",
    "Then we just need to use the len() function to gather the length of the list of chars, list of words, and the set of words. The set() does not allow repeat elements in\n",
    "the data structure, so applying this to the `words` list will supply only unique elements. Here is a sample of the functionality from the interpreter:\n",
    "\n",
    "```\n",
    ">>> chars = f.read()\n",
    ">>> chars\n",
    "['H','e','l','l','o',',',' ','W', ... ]\n",
    ">>> len(chars)\n",
    "18\n",
    ">>> words = chars.split() # By default split at the space\n",
    ">>> words\n",
    "['Hello,', 'World!', 'Hello,']\n",
    ">>> len(words)\n",
    "3\n",
    ">>> unique_words = set(words)\n",
    ">>> unique_words\n",
    "['Hello,', 'World!']\n",
    ">>> len(unique_words)\n",
    "2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4d0f9962",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to MarkovChain\n",
    "\n",
    "def _init_words(self):\n",
    "    \n",
    "    for filename in self.filenames:\n",
    "        with open(filename, 'r') as f:\n",
    "            chars = f.read()\n",
    "            words = chars.split()\n",
    "            unique_words = set(words)\n",
    "            self._populate(len(chars), len(words), len(unique_words))\n",
    "            self.initial_words.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b066d0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mFiles:\u001b[0m\n",
      "\u001b[1m\u001b[91m-\u001b[0m ./data/hunger_games.txt\n",
      "\u001b[1m\u001b[92m########################\u001b[0m\n",
      "\u001b[1mnum chars: \u001b[0m1607759\u001b[0m \n",
      "\u001b[1mnum words: \u001b[0m299960\u001b[0m \n",
      "\u001b[1mnum unique words: \u001b[0m26002\u001b[0m \n",
      "\u001b[1m\u001b[92m########################\u001b[0m\n",
      "\n",
      "\u001b[1mPreview of the Text:\u001b[0m\n",
      "When I wake up, the other side of the bed is cold. My fingers stretch out, seeking Prim’s warmth but finding only the rough canvas cover of the mattress. She must have had bad dreams and climbed in with our mother. Of course, she did. This is the day of "
     ]
    }
   ],
   "source": [
    "mc = MarkovChain(filenames=[HUNGER_GAMES_FILENAME],\n",
    "                 N=3,\n",
    "                 stop_characters=STOP_CHARACTERS,\n",
    "                 stop_words=STOP_WORDS\n",
    "                )\n",
    "\n",
    "mc._init_words()\n",
    "mc.display_specs()\n",
    "\n",
    "print(f'\\n{Style.BRIGHT}Preview of the Text:{Style.RESET_ALL}')\n",
    "for i in range(50):\n",
    "    print(mc.initial_words[i], end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5121a3",
   "metadata": {},
   "source": [
    "## MarkovChain._create_ngram_dict()\n",
    "\n",
    "This is where the probability between states comes in to play. Note here, when we add the next word beyond the\n",
    "n-gram (the Nth + 1 word), we allow duplicates into the list. This means we could recieve a list such as \n",
    "`[the, the, the, tiny]` where 75% of the words are `the` and 25% are `tiny`. When selecting from this list\n",
    "in the future, this means that if we select from the bucket randomly, we should see a selection of the\n",
    "word `the` approximately 75% of the time.  \n",
    "\n",
    "As for creating the dictionary of ngrams, the algorithm is fairly straightforward. Suppose we have the sentence below:\n",
    "\n",
    "`The dog is happy. The dog is quite a good boy. That is quite a smile you have there.`\n",
    "\n",
    "The algorithm iterates over a group of N + 1 words at a time:\n",
    "\n",
    "- Itr 1: `The dog is happy.`\n",
    "- Itr 2: `dog is happy. The`\n",
    "- Itr 3: `is happy. The dog`\n",
    "- . . .\n",
    "\n",
    "For each iteration, the `key` for the dictionary becomes the first N elements of the partial list. Then, at the given key,\n",
    "we append the Nth + 1 word beyond it to the bucket. For example:\n",
    "\n",
    "- <b>Itr 1:</b><br>\n",
    "    key: `The dog is`<br> bucket: `happy`\n",
    "- <b>Itr 2:</b><br>\n",
    "    key: `dog is happy.`<br> bucket: `The`\n",
    "- <b>Itr 3:</b><br>\n",
    "    key: `is happy. The`<br> bucket: `dog`\n",
    "- <b>. . .</b>\n",
    "- <b>Itr 5:</b><br>\n",
    "    key: `The dog is`<br> bucket: `happy, quite`\n",
    "\n",
    "Iteration 1 and 5 end up producing the same key for the dictionary, so `quite` is appended to the list from iteration 1. This collision will\n",
    "also occur (with a N=3) from the phrase `is quite a` as that combination of words exists twice in the example.\n",
    "\n",
    "key: `is quite a`<br>\n",
    "bucket: `good, smile`<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "97227460",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to MarkovChain\n",
    "\n",
    "def _create_ngram_dict(self):\n",
    "    n_grams = zip(*[self.initial_words[i:] for i in range(self.N + 1)])\n",
    "    for n_gram in n_grams:\n",
    "        key = n_gram[:self.N]\n",
    "        next_word = n_gram[-1]\n",
    "        self.n_grams[key] = self.n_grams.get(key, []) + [next_word]\n",
    "        \n",
    "        \n",
    "def _create_starting_ngram_list(self):\n",
    "    \n",
    "    is_valid      = lambda g: g[0] not in self.stop_words and (g[1][0].isupper() or g[1][0] in [\"'\", '\"'])\n",
    "    in_stop_chars = lambda g: g[0][-1] in self.stop_characters\n",
    "    \n",
    "    n_grams = zip(*[self.initial_words[i:] for i in range(self.N + 1)])\n",
    "    for n_gram in n_grams:\n",
    "        if in_stop_chars(n_gram) and is_valid(n_gram):\n",
    "            self.starting_n_grams.append(n_gram[1:])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416a7d12",
   "metadata": {},
   "source": [
    "## Preview Results from the N-Grams and the Entries\n",
    "\n",
    "Here is an example of the output produced from `MarkovChain._create_ngram_dict()`. We can see the output shifts one words to the right\n",
    "in the input stream (source text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fe3c3c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mPreview of N-Grams:\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('When', 'I', 'wake')\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('I', 'wake', 'up,')\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('wake', 'up,', 'the')\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('up,', 'the', 'other')\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('the', 'other', 'side')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mc.n_grams = {} # Only used because we are calling this multiple times.\n",
    "\n",
    "mc._create_ngram_dict()\n",
    "n_gram_vals = list(mc.n_grams.values())\n",
    "\n",
    "print(f'\\n{Style.BRIGHT}Preview of N-Grams:{Style.RESET_ALL}')\n",
    "for i, key in enumerate(mc.n_grams.keys()):\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(f'{Style.BRIGHT}- {Style.RESET_ALL}{Fore.LIGHTGREEN_EX}{key}{Style.RESET_ALL}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d284bb9",
   "metadata": {},
   "source": [
    "Additionally we have the buckets or entries that were assigned to each key. The first five elements in the preview of the n-gram entries correspond\n",
    "to the first five keys in the above preview. The remaining five entries are just other buckets that have more than two items inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bb3a93f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mPreview of the N-Gram Entries:\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92mup,\u001b[0m \u001b[92mup,\u001b[0m \n",
      "\u001b[1m- \u001b[0m\u001b[92mthe\u001b[0m \u001b[92mI’m\u001b[0m \n",
      "\u001b[1m- \u001b[0m\u001b[92mother\u001b[0m \u001b[92mrestraints\u001b[0m \n",
      "\u001b[1m- \u001b[0m\u001b[92mside\u001b[0m \n",
      "\u001b[1m- \u001b[0m\u001b[92mof\u001b[0m \u001b[92mof\u001b[0m \u001b[92mof\u001b[0m \u001b[92mof\u001b[0m \n",
      "\u001b[1m- \u001b[0m\u001b[92mof\u001b[0m \u001b[92mof\u001b[0m \u001b[92mof\u001b[0m \u001b[92mof\u001b[0m \n",
      "\u001b[1m- \u001b[0m\u001b[92mthe\u001b[0m \u001b[92mthe\u001b[0m \u001b[92mthe\u001b[0m \u001b[92mher\u001b[0m \n",
      "\u001b[1m- \u001b[0m\u001b[92mbed\u001b[0m \u001b[92mbargain\u001b[0m \u001b[92mtable.\u001b[0m \u001b[92mbuilding\u001b[0m \u001b[92mdome,\u001b[0m \u001b[92mbag\u001b[0m \u001b[92mlake\u001b[0m \u001b[92mSeam.\u001b[0m \u001b[92mcircle?”\u001b[0m \u001b[92mCornucopia,\u001b[0m \u001b[92mnarrow\u001b[0m \u001b[92mbed,\u001b[0m \u001b[92mfamily\u001b[0m \u001b[92mother\u001b[0m \u001b[92mbed\u001b[0m \u001b[92mtree,\u001b[0m \u001b[92mnet.”\u001b[0m \u001b[92mwarehouse.\u001b[0m \u001b[92mV,\u001b[0m \u001b[92mbed,\u001b[0m \u001b[92mtunnel,\u001b[0m \u001b[92mbridge\u001b[0m \u001b[92mhead\u001b[0m \u001b[92mbargain,\u001b[0m \u001b[92mbalcony\u001b[0m \u001b[92mhouse,\u001b[0m \u001b[92mhouse.”\u001b[0m \n",
      "\u001b[1m- \u001b[0m\u001b[92mis\u001b[0m \u001b[92mis\u001b[0m \u001b[92mand\u001b[0m \n",
      "\u001b[1m- \u001b[0m\u001b[92mhad\u001b[0m \u001b[92mreally\u001b[0m \u001b[92mbeen\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "print(f'\\n{Style.BRIGHT}Preview of the N-Gram Entries:{Style.RESET_ALL}')\n",
    "for n_gram in n_gram_vals[:5]:\n",
    "    print(f'{Style.BRIGHT}- {Style.RESET_ALL}', end='')\n",
    "    for gram in n_gram:\n",
    "        print(f'{Fore.LIGHTGREEN_EX}{gram}{Style.RESET_ALL}', end=' ')\n",
    "    print()\n",
    "    \n",
    "n_gram_new = list(filter(lambda x: len(x) > 2, n_gram_vals))\n",
    "for entries in n_gram_new[:5]:\n",
    "    print(f'{Style.BRIGHT}- {Style.RESET_ALL}', end='')\n",
    "    for entry in entries:\n",
    "        print(f'{Fore.LIGHTGREEN_EX}{entry}{Style.RESET_ALL}', end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ce6598b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mPreview of the N-Gram Starters:\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('My', 'fingers', 'stretch')\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('She', 'must', 'have')\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('Of', 'course,', 'she')\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('This', 'is', 'the')\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('I', 'prop', 'myself')\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('There’s', 'enough', 'light')\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('My', 'little', 'sister,')\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('In', 'sleep,', 'my')\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('Prim’s', 'face', 'is')\u001b[0m\n",
      "\u001b[1m- \u001b[0m\u001b[92m('My', 'mother', 'was')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mc.starting_n_grams = [] # Only used because we are calling this multiple times.\n",
    "\n",
    "mc._create_starting_ngram_list()\n",
    "print(f'\\n{Style.BRIGHT}Preview of the N-Gram Starters:{Style.RESET_ALL}')\n",
    "for n_gram in mc.starting_n_grams[:10]:\n",
    "    print(f'{Style.BRIGHT}- {Style.RESET_ALL}{Fore.LIGHTGREEN_EX}{n_gram}{Style.RESET_ALL}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77d5f40",
   "metadata": {},
   "source": [
    "An issue that I brought up in the introduction referred to memory usage of the program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d37f8e6",
   "metadata": {},
   "source": [
    "Although the total memory for the `MarkovChain` class is only around 1/100th of a gigabyte of memory, since Python does not limit the program memory,\n",
    "this is well within the bounds of running for computers. \n",
    "Still, for generating new sentences this seems quite wasteful. Although this implementation is nice to work with, the side effect is we are storing\n",
    "many duplicates of the same word, which add up in a hurry for large source texts. Additionally, the SentenceGenerator will require a lot of source text\n",
    "in order to produce consitently new sentences. So, in order to provide a decent sentence generator, we will have to sacrifice memory for this implementation. \n",
    "\n",
    "A potential solution to optimize this could be to, instead of storing\n",
    "the same word in a bucket multiple times, have an integer value to represent the likelihood of a word being chosen out of the group of potential words.\n",
    "Using this method, lets say we have an example like this:  \n",
    "\n",
    "```\n",
    "key: 'The next word'\n",
    "bucket: ['is', 'is','is','is','is','is','is','is','is','is','is','is','is','is','is', ...]\n",
    "```\n",
    "\n",
    "If we have the same word that is repeated many times, it could be more efficient to simply show that if we see the key 'the next word' that any number between\n",
    "0 and 1 we choose, the result will 100% be `is`. Because of this, we don't need to store all strings `is`, but we could simply replace the the `is` with a number\n",
    "that signifies how many instances of that word exist in the bucket. If we have 100 words that are `is` and 25 that are `found`, then we can perform the following calculation:\n",
    "\n",
    "```\n",
    "key: is, num: 100  \n",
    "key: found: num: 25\n",
    "total = sum(keys) = 125\n",
    "Likelihood of 'is': 100 / 125 = 0.8\n",
    "Likelihood of 'found': 25 / 125 = 0.2\n",
    "\n",
    "Select a number randomly between 0-1. If <num> <= 0.2, then select 'found', otherwise select 'is'.\n",
    "```\n",
    "\n",
    "By storing how many of the same word exists in a bucket, we can reduce a series of identical strings to a single integer. This would be ideal for extremely common words\n",
    "that follow a specific gram, but on the other end, this implementation could produce little effect for the below scenario:\n",
    "\n",
    "```\n",
    "n-gram key: ('The dog runs')\n",
    "n-gram bucket: ['wild', 'free', 'blindly', excitedly', 'willingly', 'dangerously', ... ]\n",
    "```\n",
    "\n",
    "In this example we have a statement: `The dog runs`. The words beyond `runs` that are in the bucket represent decorations for the sentence, or adverbs. There could be\n",
    "an extremely large amount of different adverbs used after a specific n-gram. This places the new solution in the same situation as before: too many unique words to store\n",
    "in a bucket will take up memory. I do not see a potential solution for this specific problem, as if there is a unique item, we have to store it in memory somehow. Otherwise,\n",
    "we would not be able to select said solution / word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4a2603",
   "metadata": {},
   "source": [
    "## Limitations to MarkovChain\n",
    "\n",
    "There is one aspect of the `MarkovChain` implementation that I am not proud of. The contents inside `MarkovChain.n_grams` include keys with capitalizations, which means there could be times where\n",
    "the keys match, however one key started a sentence and another was in the middle of the sentence. This limits our options of what words could be next. Since this approach already\n",
    "needs a lot of material to work effectively, this is just further limiting the variety of sentences it will be able to generate. A potential improvement and solution to this problem\n",
    "would be to make every letter in the key lower case after initializing the `MarkovChain.starting_n_grams`. This way we can maximize the potential choices for each gram. However, the\n",
    "side effect to ths approach means that we will eventually have to determine where a sentence starts and where a sentence ends. Additionally, I don't see a good way to do the same\n",
    "thing for punctuations: if we remove a punctuation, there will be no way to reasonably determine when a sentence can end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd9a055",
   "metadata": {},
   "source": [
    "# SentenceGenerator Class\n",
    "\n",
    "---\n",
    "\n",
    "Now that we have a skeleton for generating sentences, we will inherit from the `MarkovChain` to use its functionality to generate\n",
    "sentences. The purpose of this new class is to simply extend the behavior of the `MarkovChain` class. Therefore, the `SentenceGenerator.__init__()`\n",
    "method will only call the constructor of the base class and nothing more. For this report, I also imported the complete `MarkovChain` class\n",
    "for `SentenceGenerator` to inherit from as Jupyter-Notebook seemed to have some inconsistencies with inheritance when also using the `jdc` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e7db70e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MarkovChain.MarkovChain import MarkovChain\n",
    "\n",
    "class SentenceGenerator(MarkovChain):\n",
    "\n",
    "    def __init__(self, filenames, N, stop_characters=None, stop_words=None):\n",
    "        super().__init__(filenames, N, stop_characters=stop_characters, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b96401",
   "metadata": {},
   "source": [
    "## SentenceGenerator.generate_sentence [1st iteration]\n",
    "\n",
    "This is the initial draft of the generate_sentence method. It it actually generates\n",
    "understandable text and was my first solution that showed a promising output return.\n",
    "The key to making this function work was to separate the n-grams with n-grams that can\n",
    "start a sentence. These are `self.n_grams` and `self.starting_n_grams`, respectively.\n",
    "\n",
    "By intializing with an n-gram that is the beginning of a sentence,\n",
    "We can start the chain from the beginning of the sentence instead of midway through or at the end.\n",
    "This solution shows another issue: the sentences that it has generated, although somewhat coherent,\n",
    "end midway through a sentence. This issue took a lot of time to find a decent solution to. The `SentenceGenerator.generate_sentence()`\n",
    "method described below is fairly condensed and works well for beginning a sentence. Additionally,\n",
    "being able to provide a variety of length for a generated sentence is a great way to increase the variety of\n",
    "sentences to be generated. The issue is this approach does not consider how a sentence ends. It could end,\n",
    "if the length of the sentence determine conveniently lands on an ending phrase. However, most of the time,\n",
    "this does not occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4fa210f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to SentenceGenerator\n",
    "\n",
    "def generate_sentence(self):\n",
    "    \n",
    "    length_sentence = random.randint(4, 15)  \n",
    "    seed = random.choice(self.starting_n_grams)\n",
    "    output = [x for x in seed]\n",
    "    for _ in range(length_sentence):\n",
    "        word = random.choice(self.n_grams[seed])\n",
    "        seed = tuple(list(seed[1:]) + [word])\n",
    "        output.append(word)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ef916dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPreview of Generated Sentences:\u001b[0m\n",
      "\u001b[1m\u001b[31m- \u001b[0mThenheputtheringonhisfinger.\"Soyouhavegotridofus.We\n",
      "\u001b[1m\u001b[31m- \u001b[0mWefinallyknowthenameofagirlwho’sfromtheSeam,withheroliveskin\n",
      "\u001b[1m\u001b[31m- \u001b[0mButthesharppainswerefading.Therewasaloud\n",
      "\u001b[1m\u001b[31m- \u001b[0mAndtheEaglesoftheMountainsofTerrorintothe\n",
      "\u001b[1m\u001b[31m- \u001b[0mAllofthekidsaroundhere.\"Inoddednoncommittally,keepingmyeyesdownonthe\n"
     ]
    }
   ],
   "source": [
    "# Initializing SentenceGenerator Object\n",
    "sg = SentenceGenerator(filenames=[\n",
    "                                HUNGER_GAMES_FILENAME,\n",
    "                                TWILIGHT,\n",
    "                                FIFTY_SHADES_OF_GRAY,\n",
    "                                LORD_OF_THE_RINGS\n",
    "                                ],\n",
    "                    N=3,\n",
    "                    stop_characters=STOP_CHARACTERS,\n",
    "                    stop_words=STOP_WORDS,\n",
    "                    )\n",
    "\n",
    "print(f'{Style.BRIGHT}Preview of Generated Sentences:{Style.RESET_ALL}')\n",
    "for _ in range(5):\n",
    "    print(f'{Style.BRIGHT}{Fore.RED}- {Style.RESET_ALL}', end='')\n",
    "    for word in sg.generate_sentence():\n",
    "        print(word, end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1702b290",
   "metadata": {},
   "source": [
    "## SentenceGenerator.generate_sentence [2nd iteration]\n",
    "\n",
    "In this second iteration, I address the issue where the sentence ends halfway through. Additionally,\n",
    "a few times in the iteration one implementation, there is sometimes a random quotation that tries to\n",
    "start a quote or end a quote. This will also be addressed in this iteration. Testing for a quote at\n",
    "the beginning or end is also quite a difficult problem to fix in a simple way. Somehow, we have to keep\n",
    "track of the current state the generator is in (does it need to search for an ending quote, has it seen\n",
    "a closing quote but no opening quote?). The second problem is more challenging. It is pretty straightforward\n",
    "to search for a closing quote after seeing an opening, but what are we to do if we see a closing quote?\n",
    "\n",
    "\n",
    "To be honest, I don't have a good solution to this issue. One \"solution\" would be to eliminate the quotes\n",
    "all-together from the generator, but that is no fun. The next solution could be to insert the starting quote\n",
    "at the start of the previous sentence, but there are too many conditions to consider. For example,\n",
    "\n",
    "```\n",
    "- She said, \"Hello, foo! How is bar?\"\n",
    "- \"Hello, foo!\" she said, \"How is bar?\"\n",
    "- \"Hello, foo! How is bar?\" She said.\n",
    "```\n",
    "\n",
    "In the first example, we can't just insert the start quote at the beginning of the sentence, as that would\n",
    "be incorrect. Additionally, the second condition is even harder, for we have to potentially insert two quotes\n",
    "in one sentence! The last example would be the only time where the \"fix\" would work as intended. The problem\n",
    "with the first two examples are the word \"said\" or \"she\" can be replaced with too many different words such\n",
    "as \"He\", \"Jared\", or \"exlaimed\", \"cried.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6f408518",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to SentenceGenerator\n",
    "\n",
    "def generate_sentence(self, len: int=None):\n",
    "    \n",
    "    length_sentence = random.randint(4, 15) if len is None else len\n",
    "    seed = random.choice(self.starting_n_grams)\n",
    "    \n",
    "    \n",
    "    output = [x for x in seed]\n",
    "    is_quote = reduce(lambda base, word: (word[0] == FULL_QUOTE) or base, output, False)\n",
    "\n",
    "    for _ in range(length_sentence):\n",
    "        word, seed = self._generate_word(seed, is_quote)\n",
    "        output.append(word)\n",
    "        \n",
    "    self._end_sentence(output, seed, is_quote)\n",
    "    return ' '.join(output).rstrip()\n",
    "\n",
    "\n",
    "def _generate_word(self, seed, is_quote=False):\n",
    "    \n",
    "    not_ending_quote = lambda word: word[-1] != FULL_QUOTE\n",
    "    \n",
    "    words = self.n_grams[seed]\n",
    "    words = [word for word in words if not_ending_quote(word)] if not is_quote else words\n",
    "\n",
    "    # This is needed. If the text is primarily quotes it will make empty list.\n",
    "    if not len(words):\n",
    "        words = self.n_grams[seed]\n",
    "\n",
    "    word = random.choice(words)\n",
    "    seed = tuple(list(seed[1:]) + [word])\n",
    "    \n",
    "    return word, seed\n",
    "    \n",
    "\n",
    "def _end_sentence(self, output, seed, is_quote=False):\n",
    "    \n",
    "    in_stop_characters = lambda word: word[-1] in self.stop_characters\n",
    "    in_stop_words      = lambda word: word in self.stop_words\n",
    "    \n",
    "    while not ((end := [word for word in self.n_grams[seed] if in_stop_characters(word) and not in_stop_words(word)]) and not is_quote):\n",
    "        word, seed = self._generate_word(seed, is_quote)\n",
    "        is_quote |= word[0] == FULL_QUOTE  # if quote at beginning, make true.\n",
    "        is_quote &= word[-1] != FULL_QUOTE # if quote at end, make false.\n",
    "        output.append(word)\n",
    "        \n",
    "    word = random.choice(end)\n",
    "    output.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "acf0c5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPreview of Generated Sentences [Iteration 2]:\u001b[0m\n",
      "\u001b[1m\u001b[31m- \u001b[0mA large teardrop hangs above the barbed wire. As I look down, fascinated, as my ensemble slowly comes to life, first with a soft top.\n",
      "\u001b[1m\u001b[31m- \u001b[0mTo just press a few buttons on the steering wheel.\n",
      "\u001b[1m\u001b[31m- \u001b[0mPeeta’s alive. And a traitor. But at the same time.\n",
      "\u001b[1m\u001b[31m- \u001b[0mI just don’t understand. Christian re-enters the room.\n",
      "\u001b[1m\u001b[31m- \u001b[0mBut I know how to start. Worthless. I’m worthless. At a few minutes of effortless waltzing. \"You don't look very tan.\" \"My mother is part albino.\" He studied my face apprehensively, and I sighed.\n"
     ]
    }
   ],
   "source": [
    "print(f'{Style.BRIGHT}Preview of Generated Sentences [Iteration 2]:{Style.RESET_ALL}')\n",
    "for _ in range(5):\n",
    "    sentence = sg.generate_sentence()\n",
    "    print(f'{Style.BRIGHT}{Fore.RED}- {Style.RESET_ALL}{sentence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff4a6b6",
   "metadata": {},
   "source": [
    "This is producing better results. There are still little issues regarding quotations, but it is more consistent than before. Let's now make multiple sentences to produce\n",
    "a paragraph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bf477f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to SentenceGenerator\n",
    "\n",
    "def generate_paragraph(self, len: int=None):\n",
    "\n",
    "    num_sentences = random.randint(5,20)\n",
    "    output = []\n",
    "    for _ in range(num_sentences):\n",
    "        output.append(self.generate_sentence())\n",
    "    return ' '.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cc702511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want him presented as your new lover?” Coin asks. She hasn’t said this with such assured authority that — even though she pursed her lips — the nurse didn't argue it further. Suddenly it gave way to a man than a boy, and his voice was quite soft and un-alarming. He made no answer, he was gazing intently at me. “She distracted me from this train of thought; he was gazing intently at me. I knew I was getting the schedule of my days and nights that they had passed there. I think about leaving, but I really need you to do four.” “I thought this was a compliment. The bleakness did not entirely leave his eyes. \"That's not the worst actor in the squad. Not by a long counter, cluttered with wire baskets full of papers and brightly colored flyers taped to its front. There were three desks behind the counter, one of which was manned by a large, red-haired woman wearing glasses. She was wearing a tuxedo made me very nervous. Not quite as nervous as the dress. Or the shoe. Only one shoe, as my other foot was still securely encased in plaster. But the stiletto heel, held on only by satin ribbons, certainly wasn't going to be in Seattle that day.\" \"Oh,\" he said. Do we even have sponsors? Yes, I feel certain he would never leave his family. He picked up his lilies, and then with a beckoning wave of his hand, and walked off at a run with the silver Sunday spoon. He knows that it is our part to drive him away? My fingers encircle a blackberry and pluck it from its stem. Jackson tells me to leave a message. \"Mom,\" I said after the beep, \"it's me. Listen, I need you both. It was only in the mouths of Anduin. But Sauron of Mordor assailed them, and they squeaked all round, neek-breek, breek-neek, unceasingly all the night, until the hobbits were decent and prosperous, and no more rustic than most of those human desires are there, just hidden behind more powerful desires.\" \"Oh,\" was all I wanted, needed. I can reach you. Perhaps I am watching you now. Are there Capitol hoverplanes speeding in to blow us out of doorways, through cracks in shutters. If...\" I paused. \"There are conditions?\" He raised one eyebrow. Mithril! I have never seen President Snow. He attends celebrations in the Capitol. Suddenly I’m worried that I didn’t pounce on this information sooner, when I was dead. His email makes me weep more. I am not worried at the moment. Just give me news of my friends, and tell me he’s seen a lot worse injuries among the soldiers when they teach choke holds in training. Then he leaned forward and reached out with his long index finger presses the button summoning the elevator, and I don’t care. That I’m more than just a piece in my private Games? That’s despicable, but I’m not sure. Suddenly under the trees they seemed to catch his breath in horror. \"Edward, you have to accept. \"Is there no other way?\" he said. \"What do you think?\" he asked. \"One thing's for sure, I'll never be able to see the sun. I snap off hanging vines and use my newfound knowledge of Haymitch’s and Finnick’s treatment, all I can to aid you, each according to his stature. It is not far, but our path may be winding, for here Aragorn cannot guide us; he has seldom walked in the door has been locked from the outside. I suspect Haymitch initially, but then there’s a woman reading a list of kills. I guess technically I’d get credited for Glimmer and the girl from District 2 send a spear through a dummy’s heart from fifteen yards. “What cakes?” “At home. The iced ones, for the bakery,” he says. He kisses me one last time and goes back to singing and dunking her coil. “Oh, she’s more than smart,” says Beetee. “She’s intuitive.” We all turn and head back out to the side, and let me use the paddle!\" With a few inches at a time.\n",
      "\n",
      "I curl up, make myself smaller, try to disappear entirely. Wrapped in silence, I slide my tray along the metal shelf before the vats of food, I see breakfast is its usual dependable self — a bowl of strawberries. Sam gave various support- ing nods and exclamations. Frodo remained silent. \"I should think you were making it all up,\" said Merry, \"if I had not seen that black shape on the landing-stage – and heard the queer sound in Maggot's voice. What do you think he won the Games?” I say. Peeta shakes his head when he sees the wall of the flowerbed, barely holding me up - vomit- ing profusely is exhausting. Grey takes his hands off me. “For saving me,” I whisper. “Show you?” “Show me how you stopped... now I want to get five miles into the woods alone.\" I stared at him. I look for Peeta, but he must have the audience convinced it was to watch. Will he ever give me a smile and I’d return it without hesitating now. Mayor Undersee’s wife. Who spends half her life in bed immobilized with terrible pain, shutting out the world. I didn't even realize when I first walked in that class had already started. I bang into the room and you serve yourself. The Career Tributes tend to gather rowdily around one table, as if to hide. So much more powerful than I even recall. If anything, he seems to make her see reason. Take her for a long moment. I realize that I’m staring. I swallow. That was inside me! Leaning down, I grab my phone. Five missed calls and one voice message. Tentatively, I listen to the words. Are you with him?” “José, I’ll call you later. I can’t talk about this. She awoke alone. Whoever made her walked away, and called Sam to him. From tension to relief to something else: a look that says to leave him alone. Anyone would have thought I was far too stressed to sleep, so I did something I'd never done before. I guess the real question is what he thinks goes on in 13 as well. Not with that leg in the night. People will kill first and wonder about their motives later. He lives alone, no wife or children, most of his waking hours drunk. I don’t want to cry. Where there are so many things we should discuss, but I have managed it at last.\" \"Then what happened after Bilbo escaped from him? Do you know how to distract a man, Anastasia.” “We aim to please, Miss Steele.” His voice is filled with concern. “I’m not the strange one, you are,” I accuse. There - that told him, my courage fuelled by alcohol. “Anastasia, have you been crying? You never cry,” she says, her voice softening. She stands, her green eyes brimming with concern. She puts her arms around me. I give him another half an hour ago.\" \"What happened at the Ford?\" said Frodo. There is time yet. Wait!\" \"And I waited. Until that night when he left this house. My mom met Husband Number Three when I was little.\n",
      "\n",
      "That venture is desperate: as much so for eight as for three or two, or one alone. Boggs said you would take to win it. Or what would happen if we did. Plutarch tries to lay it out in the open. I tell Haymitch I’ll try, even though I looked down at me. “Why the fuck didn’t you tell me?” he growls. Twelve- through eighteen-year-olds are herded into roped areas marked off by ages, the oldest in the Marish or indeed in the Shire, besides you Frodo, that has ever been tortured. \"Our last short cut through the Forest! But perhaps the rose didn’t seem noteworthy to them. Only to me. Downstairs, I snag the game bag loaded with supplies from my shoulder. “It’s them. It’s all of them. The color of my dress last year. “I’m sure Caesar will ask you. And if he were merely an invention of my imagination. \"So,\" Mike said, looking at himself in a blanket. \"It won't be so hard again,\" he said with mock solemnity. It was just noon when I got back inside. I went upstairs and got dressed for the warmer weather in a deep trough of land and they could no longer see him. My heart leaps, beginning a juddering thumping beat as he makes his way down. “Hey, Annie. Congratulations.” He puts his arm around my shoulder and turn to face him, I’m shocked to find he has his fore- head pressed against mine, his eyes closed, his breathing ragged. Christian’s eyes flicker open and gaze down at him feeling victorious. My inner goddess glares at me in disbelief. “It was your first time. Wow, Christian must really know what to say about the crowd’s refusal to applaud. The silent salute. One says that District 12 has always been a bit queer lately, to my mind. I stared back, struggling to think clearly when we’re together. They are calling my flight. I have to move faster. I have to talk with Edward?\" I asked. My path was set. I just had to follow it downhill to the place I wanted to know all my desire, and long held in keeping the only treasure that I seek. \"I will help you bear this burden, as long as anyone in District Eight can remember,” says Twill. “Really?” I try to think of this very public salute to the girl if she’s stupid enough to take the stage. Effie makes hushed, distressed comments like “Oh, not Cecelia” or “Well, Chaff never could stay out of a tree that overshadowed the road. Then he lifted his head and laughed, quietly as a whisper, but still exuberantly. \"Easy for you!\" he amended, touching my nose with his fingertip. With both of us would just pick a comfortable tree to hang out in until the power shut off, which it always did eventually. If he could’ve seen what was on the brink of the fosse.\n",
      "\n",
      "He puts on the condom. I watch fascinated, mesmerized. We got a sight of anything beyond the wood's borders, though they did not exist. After an hour or more when Sam stopped a moment as if listening. Frodo became aware that spies of many sorts, even beasts and birds, were gathered round the Shire, and in the middle of this. I miss him so badly it hurts. a group of twenty rebel soldiers follow them, they’re blown to bits in a mine explosion. After a while Boromir returned carrying Sam. Behind in the narrow but now well-trodden track came Gandalf, leading Bill with Gimli perched among the baggage. Haymitch is still determined to carry on your backs?\" \"As much as we can get you some ice for your forehead, dear,\" she said to me, and I’m confused as a result. It takes a while to get your hands on the bow I know something has gone amiss. Not when they walk, not when they eat. I doubt he can hang on for a while. Jack, on the other side of the Cornucopia, snags the green backpack, and speeds off. Foxface! Mrs. Robinson. You talk to her?” I prompt, trying to rein in his temper. Finnick is talking about how they’d opened a can of chicken and rice soup and hand it to me. It was Thanksgiving, and I was afraid to be in a strange, continual twilight. Only a few yards away. And then she’s doing her odd little dance back out of the past.\n",
      "\n",
      "He wants to make me smile since we left home, or he'd be so terrified he'd just fling the Ring in his hand, “you will not be noticed. He laughed, taking my hand, leading me away from his face. I brace myself for the blow. I am hop- ing for a window seat if at all possible. “Okay, Miss Steele. You’ve been upgraded to first class.” “What?” “Ma’am, if you’d like to go to Paris? Come to think of a bird. I pick up a heavy ball and throw it a couple of chests. Alice had left her position and was running, or dancing, toward us. She hurtled to a fluid stop at our feet. Mike seemed eager as he put something back in his eyes. He could smell the unbearably sweet fragrance coming off your skin... And you and Peeta,” says Rue. “That’s eight. Wait, and the boy I thought was named Ben gathered broken branches of driftwood from the drier piles against the forest edge, and soon had a teepee-shaped construction built atop the old cinders. Will that be sufficient?” “We can always work him in as your cousin,” says Fulvia. \"Please, Bella.\" \"Why?\" I demanded. \"Trust me,\" he purred. I took another big bite. \"It was a nice day,\" he agreed. His tears at the station. Volunteering to wash Haymitch but then challenging him this morning when apparently the nice-guy approach had failed. Desolation hit me with crippling strength. I shambled along behind Jessica, not bothering to pretend to be happy for them. A slow, sexy smile spreads across his face. A nurse came bustling in long before they thought of ringing. Much too vigorously; for he came down, bang, into a tray full of plates. The faint scent of a skunk cuts through the smoke. The eyes of some animal peer at me from the kitchen. \"I'm... sorry... Edward,\" I whispered. I knew he thought it was going to be happy.” He takes a step towards me wearing his sexy predatory look. “I want you, Anastasia. Now. And you want me. You wouldn’t be sitting here with the sore ass. I don’t like it, but I don’t question this. The Hunger Games aren’t a beauty contest, but the best-looking tributes always seem to pull more sponsors. “All right, I’ll keep smiling pleasantly and you talk,” says Peeta. It’s crossed my mind, too. Repeatedly. But while I know I’ll never reach. Just to wipe my nose. He gazes at me with watchful eyes, much as he had before, when he was on his feet but not as hard as I can contrive, to those whom I know in that moment that I would feel when I looked up to see the sunset. It’s a spectacular yellow and orange blaze behind the skyline of the Capitol. The boy was speared and killed. The redheaded girl, I found out when I went into his study and closed the door before they were gone. I stood in the middle of making up a song. I was stuck over a line or two, and had prob- ably heard rumours long ago about Bilbo's vanishing. It would bring the name of Baggins must not be mentioned. I am Mr. Underhill, if any name must be given. That interested me so much that I followed them here. I slipped over the gate just behind them. Maybe Mr. Baggins has gone away. Went this morning, and the clouds had dried up, and the sheet pools at my waist, tugging my nipples, and my hands tighten on the exquisitely carved post. His hands drop away, and I hear them coming out. Back, forth, back, forth! The tracker jackers begin to buzz and I hear nothing but the vague shadowy shapes coming towards them. Maggot jumped down and stood holding the ponies\" heads, and peering forward into the light. They’re nightlock. You’ll be dead before they reach the Ferry. I’ll say this for Caesar, he really does want to feed the world. I can’t guess what form my punishment will take, how wide the net will be cast, but when it became plain that Frodo would follow Aragorn, wherever he went, he gave in. Peeta rolls his eyes at me. “Mia, calm down,” Grace admonishes softly. Come back! Leave your game and sit down beside me! We must talk a while more, and think about the morning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(sg.generate_paragraph())\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3714a21f5b22468d5a06a5f8f5b645d0465e1d7e98e037e9d760cd6d4c46ee2f"
  },
  "kernelspec": {
   "display_name": "Cpts-315-Markov-Chain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
